---
layout: post
---
<div>
<table>
    <td width="30%">
        <img src="web_page/Research_files/2021_loam/Our.png"  width="400" style="border-style:none">
        <p></p>
        <img src="web_page/Research_files/2021_loam/video.gif"  width="400" style="border-style: none">
    </td>
    <td width="70%" valign="top">
        <p style="margin-top: 0px;">
            <heading><b>A robust, high-precision LiDAR odometry and mapping with Robot Kinematics Constraint</b></heading>
        </p>
        <p style="margin-top: -0px;">
            In this work, we present a robust, high-precision LOAM algorithm for small FoV 3D LiDARs (such as, Livox sensors).
            In contrast to the popular use of inertial measurements,
            we primarily involve the robot kinematics model as a prior constraint in LOAM algorithm.
            The reason is that during some movements of robots
            (e.g. straight one with low constant speed, sudden starting/braking), 
            the pre-integration of IMU dominated by signal noise and bias.
            By taking effort on both front-end and back-end, our proposed algorithm achieves better performance 
            in both robust and precision compared to existing baselines. 
        </p>
        <input id="toggle4" type="checkbox" checked class="toggle">
        <label for="toggle4" style="margin-top: -0px;">[YouTube Video]</label>
        <div class="expand">
            <section>
                <iframe width="620" height="460" src="https://www.youtube.com/embed/jUF7lvzcWSw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </section>
        </div>
    </td>
</table>
<table>
    <td width="30%">
        <img src="web_page/Research_files/2021_robio/RoomReconstruction.png"  width="400" style="border-style:none">
        <p></p>
        <img src="web_page/Research_files/2021_robio/video.gif"  width="400" style="border-style: none">
    </td>
    <td width="70%" valign="top">
        <p style="margin-top: 0px;">
            <heading><b>Low-Drift RGB-D SLAM with Room Reconstruction Using Scene Understanding</b></heading>
        </p>
        <p style="margin-top: -0px;">
            <b>Zefeng Ye</b>, Xin Jiang, Yun-Hui Liu
        </p>
        <p style="margin-top: -0px;">
            <i>IEEE International Conference on Robotics and Biomimetics, 2021</i> (Accepted)
        </p>
        <p style="margin-top: -0px;">
            In this work, we present an architecture for online, incremental room reconstruction
            which combines an accurate RGB-D SLAM and room layout understanding.
            We proposed an efficient scene understanding method, which detects room's corners to 
            infer the wireframes and layout planes of room from single RGB-D image, even if the parts of the room are occluded.
            Moreover, the 3D global features (wireframes and layout planes of the building)
            can also improve the accuracy of state estimation, especially in geometric indoor environments.
            These 3D global features are treated as global consistent landmarks,
            it efficiently bounds the trajectory drift with the
            travel length increasing. On a public ICL-NUIM dataset,
            our algorithm achieves higher accuracy than other state-of-arts, and it also builds a
            geometrically meaningful map.
        </p>
        <!-- <p style="margin-top: -20px;">
            <a href="https://youtu.be/pTuK6SmZ3As">[Video]</a>
            <a href="web_page/Research_files/2021_robio/2021_robio.pdf">[PDF]</a>
        </p> -->
        <!-- <p style="margin-top: -20px;"> 
            <div>
                <a id="hide1" href="#hide1" class="hide">+ [YouTube Video]</a>
                <a id="show1" href="#show1" class="show">- [YouTube Video]</a>
                <a href="web_page/Research_files/2021_robio/2021_robio.pdf">[PDF]</a>
                <div class="details">
                    <iframe width="640" height="480" src="https://www.youtube.com/embed/pTuK6SmZ3As" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
            </div>
        </p> -->
        <input id="toggle3" type="checkbox" checked class="toggle">
        <label for="toggle3" style="margin-top: -0px;">[YouTube Video]</label>
        <div class="expand">
            <section>
                <iframe width="620" height="460" src="https://www.youtube.com/embed/pTuK6SmZ3As" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </section>
        </div>
        <a href="web_page/Research_files/2021_robio/2021_robio.pdf">[Preprint PDF]</a>
    </td>
</table>
<table width="100%">
    <td width="30%">
        <img src="web_page/Research_files/2021_tmech/Overview.png" width="400" style="border-style: none">
        <p></p>
        <img src="web_page/Research_files/2021_tmech/video.gif" width="400" style="border-style: none">
    </td>
    <td width="70%" valign="top">
        <p style="margin-left: 0px; margin-right: 30px;">
            <heading><b>Development of an Automatic Robot for Interior Wall Painting</b><i> (Under Review) </i></heading>
        </p>
        <p style="margin-top: -0px;">
            Yang Zhou, <b>Zefeng Ye</b>, Linzhu Yue, Linhai Gui, Xin Jiang, Xiang Li, Peng Li, Yun-hui Liu
        </p>
        <p style="margin-top: -0px;">
            In this work, we propose a full-coverage wall painting framework for the automatic interior painting robot, 
            which ables automatic perception in the construction site through 3D LiDAR and performs full-coverage painting planning 
            with smooth execution trajectories. A modeling algorithm is designed to establish the relationship 
            between the building point cloud and painting coverage, 
            it allows detecting layout planes and corners as well as sampling candidate 3D coverage waypoints by the spray gun's sector field. 
            Moreover, a novel painting coverage planner which generates optimal robot base poses and manipulator trajectories 
            is proposed for fully painting the walls. We conduct experiments on a realistic interior painting robot 
            to validate the performance of the developed framework (painting path reduced: 56.2%, painting efficiency: 40sec/m<sup>2</sup>).</p>
        <!-- <p style="margin-top: -20px;">
            <a href="https://youtu.be/QkSO2NRF3Wo">[Video]</a>
        </p> -->
        <!-- <p style="margin-top: -20px;"> 
            <div>
                <a id="hide2" href="#hide2" class="hide">+ [YouTube Video]</a>
                <a id="show2" href="#show2" class="show">- [YouTube Video]</a>
                <div class="details">
                    <iframe width="640" height="480" src="https://www.youtube.com/embed/QkSO2NRF3Wo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
            </div>
        </p> -->
        <input id="toggle2" type="checkbox" checked class="toggle">
        <label for="toggle2" style="margin-top: -0px;">[YouTube Video]</label>
        <div class="expand">
            <section>
                <iframe width="620" height="460" src="https://www.youtube.com/embed/QkSO2NRF3Wo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </section>
        </div>
    </td>
</table>
<table width="100%">
    <td width="30%">
        <img src="web_page/Research_files/2021_cyber/PlanView.png" width="400">
        <p></p>
        <img src="web_page/Research_files/2021_cyber/video.gif" width="400">
    </td>
    <td width="70%" valign="top">
        <p>
            <heading><b>A Wall Defects Based Semantic SLAM System for
                        Autonomous Interior Finishing Robot</b><i> (Under Review) </i></heading>
        </p>
        <p style="margin-top: -0px;">
            <b>Zefeng Ye</b>, Changheng Sun, Xin Jiang, Yexi Chen, Honghai Zhang, Yun-Hui Liu
        </p>
        <p style="margin-top: -0px;">
            Facing the aging problem, it is widely recognized that deploying autonomous 
            interior finishing robots is the efficient solution for the construction industry.
            However, realizing the necessary autonomy in construction scenarios
            represents a big challenge because of the complexity of the construction environment 
            (drastic illumination variation, monotonous texture on the walls, uneven floor, etc.).
            In this paper, we propose a fully-integrated robot-centric semantic SLAM system for the mobile manipulator robots.
            It enables the robots to achieve globally accurate localization along with wall defects awareness mapping. 
            These functions greatly benefit their navigation in complex construction workspace.
            A novel state estimator based on tightly coupled sensor fusion is proposed.
            It jointly optimizes the robotic states and wall-defect poses with heterogeneous constraints
            (stereo camera, 2D LiDAR, wheel-encoders, kinematics, and prior BIM map)
            in a factor graph. 
            The estimator provides the global accuracy and robustness in pose estimation of end-effector,
            even without loop closure.
            For quality inspection of walls,
            a centimeter-level wall defects-based semantic mapping is proposed to detect and register wall defects
            to the prior map of the construction site.
            By improving the accuracy of object-level mapping for wall defects with small size, this paper proposed a
            pixel-wise iterative optimization-based triangulation method and a 2D/3D IoU-based association method
            to fill the gap between the object-level SLAM with wall defects.
            The proposed system is validated in both simulation and real construction sites.
            The experimental results proved its effectiveness.
        </p>
        <!-- <p style="margin-top: -20px;"> 
            <div>
                <a id="hide3" href="#hide3" class="hide">+ [YouTube Video]</a>
                <a id="show3" href="#show3" class="show">- [YouTube Video]</a>
                <div class="details">
                    <iframe width="640" height="480" src="https://www.youtube.com/embed/wcEjymOD_18" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                </div>
            </div>
        </p> -->
        <input id="toggle1" type="checkbox" checked class="toggle">
        <label for="toggle1" style="margin-top: -0px;">[YouTube Video]</label>
        <div class="expand">
            <section>
                <iframe width="620" height="460" src="https://www.youtube.com/embed/wcEjymOD_18" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </section>
        </div>
    </td>
</table>
</div>
